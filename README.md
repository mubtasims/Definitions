# Definitions

Software Delivery Circle: It refers to a methodology with clearly defined processes for creating high-quality software. The cycle focuses on Requirement analysis, Planning, Software design such as architectural design, Software development, Testing, Deployment. A process that produces software with the highest quality and lowest cost in the shortest time possible. SDLC provides a well-structured flow of phases that help an organization to quickly produce high-quality software which is well-tested and ready for production use. SDLC works by lowering the cost of software development while simultaneously improving quality and shortening production time. SDLC achieves these apparently divergent goals by following a plan that removes the typical pitfalls of software development projects. That plan starts by evaluating existing systems for deficiencies. Next, it defines the requirements of the new system. It then creates the software through the stages of analysis, planning, design, development, testing, and deployment. By anticipating costly mistakes like failing to ask the end-user or client for feedback, SLDC can eliminate redundant rework and after-the-fact fixes. It’s also important to know that there is a strong focus on the testing phase. As the SDLC is a repetitive methodology, you have to ensure code quality at every cycle. Many organizations tend to spend few efforts on testing while a stronger focus on testing can save them a lot of rework, time, and money. Be smart and write the right types of tests.

Test-driven development: It refers to a style of programming in which three activities are tightly interwoven: coding, testing (in the form of writing unit tests) and design (in the form of refactoring). It is a software development approach in which test cases are developed to specify and validate what the code will do. In simple terms, test cases for each functionality are created and tested first and if the test fails then the new code is written in order to pass the test and making code simple and bug-free. The procedure starts with designing and developing tests for every small functionality of an application. It also instructs developers to write new code only if an automated test has failed which avoids duplication of code.

Continuous Integration: Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day. Each integration can then be verified by an automated build and automated tests. While automated testing is not strictly part of CI it is typically implied.can detect errors quickly and locate them more easily. As each change introduced is typically small, pinpointing the specific change that introduced a defect can be done quickly.

Continuous Delivery: Continuous Delivery is the ability to get changes of all types—including new features, configuration changes, bug fixes and experiments—into production, or into the hands of users, safely and quickly in a sustainable way. It has low risk releases, faster time to market, higher quality, lower costs, which pushes out better products with a happy team.

Configuration management: Configuration management is a form of IT service management (ITSM) as defined by ITIL that ensures the configuration of system resources, computer systems, servers and other assets are known, good and trusted. It's sometimes referred to as IT automation. Most configuration management involves a high degree of automation to achieve these goals. This is why teams use different tools like Puppet, Ansible, Terraform and other configuration management tools. By using automation, it's easier to build in checks and redundancies, improving the potential for omissions due to human error and the accuracy for keeping assets in the desired state. 

Containerization: Containerization means bundling an application together with all of its related configuration files, libraries and dependencies required for it to run in an efficient and bug-free way across different computing environments. It has become a major trend in software development as an alternative or companion to virtualization. It involves encapsulating or packaging up software code and all its dependencies so that it can run uniformly and consistently on any infrastructure. The technology is quickly maturing, resulting in measurable benefits for developers and operations teams as well as overall software infrastructure.

Cloud Scalability, and Reliability: Scalability in cloud computing is the ability to quickly and easily increase or decrease the size or power of an IT solution. One can scale up and down a cloud service pretty easily, effectively, and efficiently. Data storage capacity, processing power and networking can all be scaled using existing cloud computing infrastructure. Better yet, scaling can be done quickly and easily, typically with little to no disruption or down time. Third-party cloud providers have all the infrastructure already in place; in the past, when scaling with on-premises physical infrastructure, the process could take weeks or months and require tremendous expense.Cloud Reliability on the other hand talks about how reliable the cloud systems are, both while scaling up and down and also during general times. Cloud computing allows engineers to rapidly develop complex systems and deploy them continuously, at global scale which can create unique reliability risks. That's why cloud infrastructure providers are constantly developing new ideas and incorporating them into their products and services in order to increase the reliability of their platforms. Cloud application developers can also employ a number of architectural techniques and opera-tional best practices to further boost the availability of their systems.
